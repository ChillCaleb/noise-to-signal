{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320bf05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, io, math, textwrap\n",
    "from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dtparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "try:\n",
    "    from readability import Document\n",
    "except ModuleNotFoundError:\n",
    "    from readability.readability import Document\n",
    "import trafilatura\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "\n",
    "SAFE_UA = \"noise-to-signal/0.1 (+https://github.com/ChillCaleb/noise-to-signal)\"\n",
    "MAX_HTML_BYTES = 2_000_000\n",
    "MAX_PDF_BYTES  = 10_000_000\n",
    "CONNECT_TO = 4\n",
    "READ_TO    = 12\n",
    "\n",
    "TRACKING_KEYS_PREFIX = (\"utm_\",)\n",
    "TRACKING_KEYS_EXACT = {\"gclid\",\"fbclid\",\"igshid\",\"mc_cid\",\"mc_eid\",\"ref\",\"spm\",\"yclid\",\"ocid\",\"cmpid\",\"campaign\",\"adid\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c593c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_url(raw: str) -> str:\n",
    "    u = urlparse(raw.strip())\n",
    "    scheme = \"https\" if u.scheme in (\"http\",\"https\") else \"https\"\n",
    "    kept = []\n",
    "    for k, v in parse_qsl(u.query, keep_blank_values=True):\n",
    "        if k in TRACKING_KEYS_EXACT or any(k.startswith(p) for p in TRACKING_KEYS_PREFIX):\n",
    "            continue\n",
    "        kept.append((k, v))\n",
    "    query = urlencode(kept, doseq=True)\n",
    "    return urlunparse((scheme, u.hostname or \"\", u.path or \"/\", \"\", query, \"\"))\n",
    "\n",
    "def _soup(html: str) -> BeautifulSoup:\n",
    "    try:\n",
    "        return BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def pick_first(*vals):\n",
    "    for v in vals:\n",
    "        if v:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "def guess_language_from_html(html: str) -> Optional[str]:\n",
    "\n",
    "    m = re.search(r'<html[^>]*\\blang=[\"\\']?([a-zA-Z-]+)', html, flags=re.I)\n",
    "    return m.group(1).lower() if m else None\n",
    "\n",
    "_SENT_END = re.compile(r'(?<=[.!?])\\s+(?=[A-Z0-9â€œ\"])')\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    text = re.sub(r'\\s+', ' ', (text or '').strip())\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = _SENT_END.split(text)\n",
    "    # guard against huge blobs\n",
    "    return [s.strip() for s in parts if s.strip()][:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67feb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url: str):\n",
    "    s_url = sanitize_url(url)\n",
    "    headers = {\"User-Agent\": SAFE_UA, \"Accept\": \"*/*\"}\n",
    "    with requests.get(s_url, headers=headers, timeout=(CONNECT_TO, READ_TO), allow_redirects=True, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        cap = MAX_PDF_BYTES if \"pdf\" in ctype or r.url.lower().endswith(\".pdf\") else MAX_HTML_BYTES\n",
    "        chunks, size = [], 0\n",
    "        for chunk in r.iter_content(16384):\n",
    "            if not chunk: break\n",
    "            size += len(chunk)\n",
    "            if size > cap: break\n",
    "            chunks.append(chunk)\n",
    "        return r.url, b\"\".join(chunks), r.headers\n",
    "\n",
    "def extract_from_html(raw_html: bytes, final_url: str) -> dict:\n",
    "    html = raw_html.decode(\"utf-8\", errors=\"ignore\")\n",
    "    soup = _soup(html)\n",
    "\n",
    "    og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "    og_desc  = soup.find(\"meta\", property=\"og:description\")\n",
    "    meta_desc = soup.find(\"meta\", attrs={\"name\":\"description\"})\n",
    "    meta_time = soup.find(\"meta\", property=\"article:published_time\")\n",
    "    time_tag  = soup.find(\"time\")\n",
    "\n",
    "    title = pick_first(og_title and og_title.get(\"content\"), soup.find(\"title\").text.strip() if soup.find(\"title\") else None)\n",
    "    desc  = pick_first(meta_desc and meta_desc.get(\"content\"), og_desc and og_desc.get(\"content\"))\n",
    "    published_raw = pick_first(meta_time and meta_time.get(\"content\"), time_tag and time_tag.get(\"datetime\"))\n",
    "\n",
    "    canon = soup.find(\"link\", rel=lambda x: x and \"canonical\" in x)\n",
    "    canonical_url = sanitize_url(canon.get(\"href\")) if canon and canon.get(\"href\") else sanitize_url(final_url)\n",
    "\n",
    "    # main text: trafilatura â†’ readability fallback\n",
    "    main_text = trafilatura.extract(html, include_comments=False, include_tables=False)\n",
    "    if not main_text:\n",
    "        try:\n",
    "            doc = Document(html)\n",
    "            main_text = _soup(doc.summary()).get_text(\"\\n\").strip()\n",
    "        except Exception:\n",
    "            main_text = None\n",
    "\n",
    "    record = {\n",
    "        \"canonical_url\": canonical_url,\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"title\": title,\n",
    "        \"description\": (desc or None),\n",
    "        \"published_ts\": (dtparse.parse(published_raw).isoformat() if published_raw else None),\n",
    "        \"language\": (guess_language_from_html(html) or \"en\"),\n",
    "        \"text\": (main_text or None),\n",
    "        \"raw_len\": len(html),\n",
    "    }\n",
    "    record[\"paywall_suspect\"] = is_likely_paywalled(record, html)\n",
    "    return record\n",
    "\n",
    "def extract_from_pdf(raw_pdf: bytes, final_url: str) -> dict:\n",
    "    try:\n",
    "        txt = pdf_extract_text(io.BytesIO(raw_pdf))\n",
    "        txt = txt.strip() if txt else None\n",
    "    except Exception:\n",
    "        txt = None\n",
    "    return {\n",
    "        \"canonical_url\": sanitize_url(final_url),\n",
    "        \"content_type\": \"application/pdf\",\n",
    "        \"title\": None,\n",
    "        \"description\": None,\n",
    "        \"published_ts\": None,\n",
    "        \"language\": \"en\",\n",
    "        \"text\": txt,\n",
    "        \"raw_len\": len(raw_pdf),\n",
    "        \"paywall_suspect\": False,\n",
    "    }\n",
    "\n",
    "# Heuristic paywall detector (lightweight & conservative)\n",
    "PAYWALL_TOKENS = {\n",
    "    \"subscribe\", \"subscription\", \"log in\", \"login\", \"account\", \"unlimited access\",\n",
    "    \"continue reading\", \"you have reached your limit\", \"gift article\", \"subscriber-only\",\n",
    "    \"metered\", \"metering\", \"paywall\", \"please enable javascript\"\n",
    "}\n",
    "def is_likely_paywalled(doc: dict, raw_html_text: str) -> bool:\n",
    "    # Very short text after extraction + typical paywall strings in the HTML\n",
    "    text_len = (len(doc.get(\"text\") or \"\") if doc else 0)\n",
    "    token_hits = sum(1 for t in PAYWALL_TOKENS if t in (raw_html_text or \"\").lower())\n",
    "    # Treat NYT as stricter: if domain contains nytimes and text is thin, flag.\n",
    "    domain = urlparse(doc.get(\"canonical_url\") or \"\").hostname or \"\"\n",
    "    nyt_bias = (\"nytimes.com\" in domain)\n",
    "    return (text_len < 1200 and token_hits >= 1) or (nyt_bias and text_len < 1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abad2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "def scan_phrases(text: Optional[str], terms: List[str]) -> Dict[str, Any]:\n",
    "    if not text:\n",
    "        return {\"hits\": [], \"coverage\": 0.0, \"n_terms\": len(terms)}\n",
    "    hay = text.lower()\n",
    "    hits, found = [], set()\n",
    "    for term in terms:\n",
    "        t = term.lower().strip()\n",
    "        if not t: continue\n",
    "        for m in re.finditer(re.escape(t), hay):\n",
    "            start, end = m.start(), m.end()\n",
    "            ctx = hay[max(0, start-80): min(len(hay), end+80)]\n",
    "            hits.append({\"term\": term, \"pos\": start, \"context\": ctx})\n",
    "            found.add(term)\n",
    "    denom = max(1, len([t for t in terms if t.strip()]))\n",
    "    return {\"hits\": hits, \"coverage\": len(found)/denom, \"n_terms\": len(terms)}\n",
    "\n",
    "# Very simple extractive summary: rank sentences by TF of non-stopwords\n",
    "STOP = set(\"\"\"\n",
    "a an the and or but if because as while of in on to from for with without within into about over under\n",
    "is are was were be been being it this that these those there here who whom whose which when where why how\n",
    "by at we you they he she i me him her them us our your their its not no nor so than then too very just\n",
    "\"\"\".split())\n",
    "\n",
    "def summarize_text(text: str, max_sentences: int = 5) -> list[str]:\n",
    "    sents = split_sentences(text)\n",
    "    if not sents:\n",
    "        return []\n",
    "    # build term frequencies\n",
    "    tf = {}\n",
    "    for s in sents:\n",
    "        for w in re.findall(r\"[A-Za-z]{3,}\", s.lower()):\n",
    "            if w in STOP: \n",
    "                continue\n",
    "            tf[w] = tf.get(w, 0) + 1\n",
    "    if not tf:\n",
    "        return sents[:max_sentences]\n",
    "    # score sentences by sum of tf of their words\n",
    "    scores = []\n",
    "    for idx, s in enumerate(sents):\n",
    "        score = sum(tf.get(w, 0) for w in re.findall(r\"[A-Za-z]{3,}\", s.lower()) if w not in STOP)\n",
    "        scores.append((score, idx, s))\n",
    "    scores.sort(reverse=True)\n",
    "    # keep top non-duplicate sentences in original order\n",
    "    chosen = sorted(scores[: max_sentences * 2], key=lambda x: x[1])  # overselect a bit, then cut\n",
    "    result = [s for _, _, s in chosen][:max_sentences]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f142be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "import os\n",
    "\n",
    "ALLOWED_NEWS_DOMAINS = {\n",
    "    \"reuters.com\", \"apnews.com\", \"cnbc.com\",\n",
    "    \"imf.org\", \"whitehouse.gov\", \"federalreserve.gov\", \"treasury.gov\",\n",
    "    \"ecb.europa.eu\", \"oecd.org\", \"un.org\"\n",
    "}\n",
    "\n",
    "def domain_amp_variant(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Common AMP patterns for big outlets.\n",
    "    Returns an AMP URL string or None if no obvious variant.\n",
    "    \"\"\"\n",
    "    u = urlparse(url)\n",
    "    host = (u.hostname or \"\").lower()\n",
    "\n",
    "    # NYT\n",
    "    if \"nytimes.com\" in host:\n",
    "        # preserve query if exists\n",
    "        q = u.query\n",
    "        if \"outputType=amp\" not in q:\n",
    "            q = (q + \"&\" if q else \"\") + \"outputType=amp\"\n",
    "        return urlunparse((u.scheme or \"https\", host, u.path, u.params, q, u.fragment))\n",
    "\n",
    "    # Washington Post\n",
    "    if \"washingtonpost.com\" in host:\n",
    "        path = u.path\n",
    "        if not path.endswith(\"/amp\"):\n",
    "            path = path.rstrip(\"/\") + \"/amp\"\n",
    "        return urlunparse((u.scheme or \"https\", host, path, u.params, u.query, u.fragment))\n",
    "\n",
    "    # The Guardian (example)\n",
    "    if \"theguardian.com\" in host:\n",
    "        path = u.path\n",
    "        if not path.endswith(\"/amp\"):\n",
    "            path = path.rstrip(\"/\") + \"/amp\"\n",
    "        return urlunparse((u.scheme or \"https\", host, path, u.params, u.query, u.fragment))\n",
    "\n",
    "    return None\n",
    "\n",
    "def slug_to_query(url: str, max_terms: int = 8) -> str:\n",
    "    \"\"\"\n",
    "    Convert URL path into a compact NewsAPI query (drop dates/stopwords).\n",
    "    \"\"\"\n",
    "    stop = set(\"the a an and or of to in on for with from at by as about into over under after before\".split())\n",
    "    path = (urlparse(url).path or \"\").strip(\"/\")\n",
    "    # split on slashes and hyphens, keep alphabetic tokens\n",
    "    parts = re.split(r\"[/-]+\", path)\n",
    "    words = []\n",
    "    for p in parts:\n",
    "        if len(p) < 3: \n",
    "            continue\n",
    "        if re.fullmatch(r\"\\d{2,4}\", p):  # likely a year/day segment\n",
    "            continue\n",
    "        w = re.sub(r\"[^A-Za-z]\", \"\", p).lower()\n",
    "        if len(w) >= 3 and w not in stop:\n",
    "            words.append(w)\n",
    "    # dedupe but keep order\n",
    "    seen = set(); terms = []\n",
    "    for w in words:\n",
    "        if w not in seen:\n",
    "            terms.append(w); seen.add(w)\n",
    "    return \" \".join(terms[:max_terms]) or path.replace(\"/\", \" \")\n",
    "\n",
    "def fetch_related_from_newsapi(query: str, page_size: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Query NewsAPI for related coverage and return lightweight items.\n",
    "    Filters to friendlier domains when possible.\n",
    "    \"\"\"\n",
    "    key = os.getenv(\"NEWSAPI_KEY\")\n",
    "    if not key:\n",
    "        return []\n",
    "\n",
    "    base = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"language\": \"en\",\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "        \"pageSize\": page_size,\n",
    "        \"apiKey\": key,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(base, params=params, timeout=(CONNECT_TO, READ_TO))\n",
    "        r.raise_for_status()\n",
    "        items = (r.json() or {}).get(\"articles\", []) or []\n",
    "\n",
    "        # Prefer allowed domains first\n",
    "        def host(u):\n",
    "            try: return (urlparse(u).hostname or \"\").lower()\n",
    "            except: return \"\"\n",
    "        allowed = [x for x in items if host(x.get(\"url\") or \"\") in ALLOWED_NEWS_DOMAINS]\n",
    "        chosen = allowed or items\n",
    "\n",
    "        out = []\n",
    "        for x in chosen[:page_size]:\n",
    "            out.append({\n",
    "                \"source\": (x.get(\"source\") or {}).get(\"name\"),\n",
    "                \"title\": x.get(\"title\"),\n",
    "                \"url\": x.get(\"url\"),\n",
    "                \"publishedAt\": x.get(\"publishedAt\"),\n",
    "                \"description\": x.get(\"description\"),\n",
    "            })\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f3935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read(url: str) -> dict:\n",
    "    try:\n",
    "        # 1ï¸âƒ£  Fetch and extract normally\n",
    "        blob, final_url, domain, is_agg = fetch_url(url)\n",
    "        doc = extract_from_html(blob, final_url)\n",
    "        doc.update({...})  # metadata, timestamps, etc.\n",
    "        return doc\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # 2ï¸âƒ£  If we get a 403 or timeout\n",
    "        if \"403\" in str(e) or \"Forbidden\" in str(e) or \"timed out\" in str(e):\n",
    "            \n",
    "            # (a) AMP fallback: look for AMP link in HTML (if we have partial content)\n",
    "            amp_url = find_amp_url(blob)\n",
    "            if amp_url:\n",
    "                try:\n",
    "                    blob, final_url, domain, is_agg = fetch_url(amp_url)\n",
    "                    doc = extract_from_html(blob, final_url)\n",
    "                    doc[\"fetched_via\"] = \"AMP\"\n",
    "                    return doc\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # (b) NewsAPI fallback: search related coverage\n",
    "            query_terms = slug_to_query(url)\n",
    "            related = fetch_related_from_newsapi(query_terms)\n",
    "            return {\n",
    "                \"blocked\": True,\n",
    "                \"source_url\": url,\n",
    "                \"related_coverage\": related,\n",
    "                \"summary_hint\": \"Original article paywalled; using related coverage.\"\n",
    "            }\n",
    "\n",
    "        # (c) Other network errors\n",
    "        return {\"error\": str(e), \"blocked\": False, \"source_url\": url}\n",
    "\n",
    "    except Exception as e:\n",
    "        # (d) Anything else\n",
    "        return {\"error\": f\"Unhandled: {e}\", \"source_url\": url}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20808e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch + extract with fallbacks:\n",
    "      1) normal fetch\n",
    "      2) AMP variant (if applicable) on 403/timeout\n",
    "      3) NewsAPI related coverage (structured) if still blocked\n",
    "    \"\"\"\n",
    "    try:\n",
    "        final_url, blob, headers = fetch_url(url)\n",
    "        ctype = (headers.get(\"Content-Type\") or \"\").lower()\n",
    "        domain = (urlparse(final_url).hostname or \"\").lower()\n",
    "\n",
    "        if \"pdf\" in ctype or final_url.lower().endswith(\".pdf\"):\n",
    "            doc = extract_from_pdf(blob, final_url)\n",
    "        else:\n",
    "            doc = extract_from_html(blob, final_url)\n",
    "\n",
    "        doc.update({\n",
    "            \"domain\": domain,\n",
    "            \"bytes\": len(blob),\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "            \"blocked\": False,\n",
    "            \"fetched_via\": \"direct\",\n",
    "        })\n",
    "        # add preview\n",
    "        if doc.get(\"text\"):\n",
    "            lines = [ln.strip() for ln in doc[\"text\"].splitlines() if ln.strip()]\n",
    "            preview = \" \".join(lines)\n",
    "            doc[\"summary_hint\"] = (preview[:600] + \"â€¦\") if len(preview) > 600 else preview\n",
    "        else:\n",
    "            doc[\"summary_hint\"] = doc.get(\"description\")\n",
    "        return doc\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = str(e).lower()\n",
    "        is_forbidden = (\"403\" in msg) or (\"forbidden\" in msg)\n",
    "        is_timeout   = (\"timed out\" in msg) or (\"timeout\" in msg)\n",
    "\n",
    "        if is_forbidden or is_timeout:\n",
    "            # 2) AMP variant try\n",
    "            amp = domain_amp_variant(url)\n",
    "            if amp:\n",
    "                try:\n",
    "                    final_url, blob, headers = fetch_url(amp)\n",
    "                    ctype = (headers.get(\"Content-Type\") or \"\").lower()\n",
    "                    domain = (urlparse(final_url).hostname or \"\").lower()\n",
    "                    if \"pdf\" in ctype or final_url.lower().endswith(\".pdf\"):\n",
    "                        doc = extract_from_pdf(blob, final_url)\n",
    "                    else:\n",
    "                        doc = extract_from_html(blob, final_url)\n",
    "                    doc.update({\n",
    "                        \"domain\": domain,\n",
    "                        \"bytes\": len(blob),\n",
    "                        \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "                        \"blocked\": False,\n",
    "                        \"fetched_via\": \"amp\",\n",
    "                    })\n",
    "                    if doc.get(\"text\"):\n",
    "                        lines = [ln.strip() for ln in doc[\"text\"].splitlines() if ln.strip()]\n",
    "                        preview = \" \".join(lines)\n",
    "                        doc[\"summary_hint\"] = (preview[:600] + \"â€¦\") if len(preview) > 600 else preview\n",
    "                    else:\n",
    "                        doc[\"summary_hint\"] = doc.get(\"description\")\n",
    "                    return doc\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # 3) Related coverage via NewsAPI\n",
    "            q = slug_to_query(url)\n",
    "            related = fetch_related_from_newsapi(q, page_size=5)\n",
    "            return {\n",
    "                \"canonical_url\": sanitize_url(url),\n",
    "                \"content_type\": \"text/html\",\n",
    "                \"title\": None, \"description\": None,\n",
    "                \"published_ts\": None, \"language\": \"en\",\n",
    "                \"text\": None, \"summary_hint\": \"Original article blocked; showing related coverage.\",\n",
    "                \"domain\": (urlparse(url).hostname or \"\").lower(),\n",
    "                \"bytes\": 0,\n",
    "                \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "                \"blocked\": True,\n",
    "                \"fetched_via\": \"newsapi_related\",\n",
    "                \"related_coverage\": related,\n",
    "                \"paywall_suspect\": True,\n",
    "            }\n",
    "\n",
    "        # Other network errors\n",
    "        return {\n",
    "            \"canonical_url\": sanitize_url(url),\n",
    "            \"content_type\": None,\n",
    "            \"title\": None, \"description\": None,\n",
    "            \"published_ts\": None, \"language\": \"en\",\n",
    "            \"text\": None, \"summary_hint\": None,\n",
    "            \"domain\": (urlparse(url).hostname or \"\").lower(),\n",
    "            \"bytes\": 0, \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "            \"blocked\": False, \"fetched_via\": \"error\",\n",
    "            \"error\": str(e), \"paywall_suspect\": False,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"canonical_url\": sanitize_url(url),\n",
    "            \"content_type\": None,\n",
    "            \"title\": None, \"description\": None,\n",
    "            \"published_ts\": None, \"language\": \"en\",\n",
    "            \"text\": None, \"summary_hint\": None,\n",
    "            \"domain\": (urlparse(url).hostname or \"\").lower(),\n",
    "            \"bytes\": 0, \"fetched_at\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "            \"blocked\": False, \"fetched_via\": \"exception\",\n",
    "            \"error\": str(e), \"paywall_suspect\": False,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a406858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "def fetch_url(url: str):\n",
    "    # Browser-y headers so basic bot filters chill\n",
    "    headers = {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/123.0.0.0 Safari/537.36\"),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\",\n",
    "    }\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.headers.update(headers)\n",
    "\n",
    "    # Robust retries for transient 403/5xx/connection resets\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.6,\n",
    "        status_forcelist=(403, 408, 429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\", \"HEAD\"])\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    r = s.get(url, timeout=(CONNECT_TO, READ_TO), allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # normalize content-type\n",
    "    ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    return r.url, r.content, {\"Content-Type\": ctype}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing open_and_read() on: https://www.federalreserve.gov/newsevents/pressreleases/monetary20250917a.htm \n",
      "\n",
      "âŒ Test failed: name 'print_record' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- TEST URL INGESTION PIPELINE (FEDERAL RESERVE PRESS RELEASE) ---\n",
    "\n",
    "test_url = \"https://www.federalreserve.gov/newsevents/pressreleases/monetary20250917a.htm\"\n",
    "\n",
    "print(\"ðŸ” Testing open_and_read() on:\", test_url, \"\\n\")\n",
    "\n",
    "try:\n",
    "    doc = open_and_read(test_url)\n",
    "\n",
    "    print_record(\n",
    "        doc,\n",
    "        terms=[\"Federal Reserve\", \"rates\", \"inflation\", \"monetary\", \"policy\", \"interest\", \"markets\"],\n",
    "        summarize=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- STRUCTURAL CHECK ---\")\n",
    "    print(\"Blocked:\", doc.get(\"blocked\"))\n",
    "    print(\"Fetched via:\", doc.get(\"fetched_via\"))\n",
    "    print(\"Content type:\", doc.get(\"content_type\"))\n",
    "    print(\"Bytes:\", doc.get(\"bytes\"))\n",
    "    print(\"Language:\", doc.get(\"language\"))\n",
    "    print(\"Title:\", doc.get(\"title\") or \"(missing)\")\n",
    "    print(\"Text length:\", len(doc.get(\"text\") or \"\"))\n",
    "\n",
    "    if doc.get(\"text\"):\n",
    "        print(\"\\n--- TEXT PREVIEW ---\")\n",
    "        print(doc[\"text\"][:400], \"...\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No text body extracted â€” may be empty or malformed HTML.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ Test failed:\", str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570360c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Restore the built-in input function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input' is not defined"
     ]
    }
   ],
   "source": [
    "# Restore the built-in input function\n",
    "del input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35b0406",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ask the user for a link\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m user_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mðŸ”— Drop link here: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Import and run ingestion\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murl_ingest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m open_and_read\n",
      "File \u001b[0;32m~/noise-to-signal/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py:1275\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noise-to-signal/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py:1320\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Ask the user for a link\n",
    "user_url = input(\"ðŸ”— Drop link here: \")\n",
    "\n",
    "# Import and run ingestion\n",
    "from url_ingest import open_and_read\n",
    "\n",
    "result = open_and_read(user_url)\n",
    "\n",
    "# Optionally show a short preview\n",
    "print(\"\\n--- RESULT PREVIEW ---\")\n",
    "print(result.get(\"text\", \"\")[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bfdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
